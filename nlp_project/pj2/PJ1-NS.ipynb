{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31468ac9-1804-4ef5-abbe-994aaa090345",
   "metadata": {},
   "source": [
    "# 基于GloVe词向量的文本分类\n",
    "> 作者：宋文彦  \n",
    "> 学号：21302010062\n",
    "\n",
    "## 实验目标\n",
    "本实验将基于GloVe词向量和PyTorch构建一个文本分类模型，使用SST2情感分析数据集进行训练、验证和测试。我们将构建以下模型：\n",
    "1. **GloVe + Transformer + Pooling + Classifier**\n",
    "2. **GloVe + RNN + Pooling + Classifier**\n",
    "\n",
    "## 实验步骤\n",
    "1. 导入PyTorch和TorchText库，加载SST2数据集\n",
    "2. 处理数据：文本数据和标签处理，构建词汇表，加载GloVe词向量\n",
    "3. 定义文本分类模型（基于GloVe + Transformer 和 GloVe + RNN 的模型）\n",
    "4. 训练模型、验证模型，并评估性能\n",
    "\n",
    "### Step 1: 导入PyTorch和TorchText库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d34dad-d343-4406-860d-6881a1d2634f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import SST2\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.transforms import VocabTransform, ToTensor\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3971f-f63a-489d-a82a-e4a697d49d50",
   "metadata": {},
   "source": [
    "### Step 2: 处理数据和标签\n",
    "\n",
    "在本节中，我们将创建自定义的文本处理器 `TextProcessor` 和标签处理器 `label_processor`。这些处理器将用于对文本进行分词、映射到词汇表索引，并将标签转换为张量。\n",
    "我们还会定义一个 `collate_fn` 函数，用于在批处理数据时对文本进行填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ba10c-c279-4378-a467-118383a1b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GloVe 词向量\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "# 文本处理器\n",
    "class TextProcessor:\n",
    "    def __init__(self, vocab, max_length=200):\n",
    "        self.vocab_transform = VocabTransform(vocab)\n",
    "        self.max_length = max_length\n",
    "        self.to_tensor = ToTensor()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # 分词并转换为小写，然后映射到词汇表索引\n",
    "        tokens = text.lower().split()\n",
    "        tokens = self.vocab_transform(tokens)\n",
    "        return self.to_tensor(tokens[:self.max_length])  # 截断到 max_length 并转为张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ed8c1-5412-46a5-8521-39ed8ccbe998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签处理器：将标签转换为张量\n",
    "def label_processor(label):\n",
    "    return torch.tensor(int(label))\n",
    "\n",
    "# 批处理函数\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = [text_processor(text) for text in texts]\n",
    "    texts = pad_sequence(texts, batch_first=True)  # 填充到相同长度\n",
    "    labels = torch.stack([label_processor(label) for label in labels])\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc9b26-79b8-4fd5-8db6-2fbe4433a8f6",
   "metadata": {},
   "source": [
    "### Step 3: 加载SST2数据集\n",
    "使用 `torchtext.datasets.SST2` 来加载Stanford Sentiment Treebank (SST2)数据集，包含训练、验证和测试集。\n",
    "我们将使用 `DataLoader` 来对数据进行批量处理，并应用我们定义的 `collate_fn`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bbe2c-5f1c-42cc-a198-5452589deaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 SST2 数据集\n",
    "train_data = list(SST2(split='train'))\n",
    "valid_data = list(SST2(split='validation'))\n",
    "test_data = list(SST2(split='test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d17c3-6299-442b-958f-95478c7237b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cf461-280c-4d05-897e-86f4e3c57cba",
   "metadata": {},
   "source": [
    "### Step 4: 构建模型\n",
    "本部分将定义文本分类模型，模型采用以下结构：\n",
    "1. **Embedding Layer**: 采用 GloVe 预训练的词向量初始化嵌入层，词向量不可训练。\n",
    "2. **Transformer 或 RNN 编码器**：用于提取句子的特征表示。\n",
    "3. **Pooling Layer**: 自适应最大池化，用于从编码后的序列中提取最重要的特征。\n",
    "4. **Classifier**: 一个简单的全连接分类器。\n",
    "\n",
    "#### 4.1 GloVe + Transformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8fa37-e6bd-4664-ad00-b66781120f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(TransformerTextClassificationModel, self).__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(glove.vectors)  # 加载 GloVe 词向量\n",
    "        self.embedding.weight.requires_grad = False  # 不训练词向量\n",
    "\n",
    "        # Transformer 编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, nhead=2, dim_feedforward=hidden_dim), num_layers\n",
    "        )\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)  # 自适应最大池化\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        encoded = self.transformer_encoder(embedded)\n",
    "        pooled = self.pooling(encoded.permute(0, 2, 1)).squeeze(-1)  # 池化并去掉多余维度\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749254d8-0399-45db-a465-9bc48ece3a11",
   "metadata": {},
   "source": [
    "#### 4.2 GloVe + RNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426bce9-1c8e-4223-ac4d-87da269d8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(RNNTextClassificationModel, self).__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN 层\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)  # 自适应最大池化\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        pooled = self.pooling(rnn_output.permute(0, 2, 1)).squeeze(-1)\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c89a0-a091-49e7-b7ea-3078339cf1cd",
   "metadata": {},
   "source": [
    "### Step 5: 创建模型、优化器和损失函数\n",
    "我们将使用交叉熵损失函数（`nn.CrossEntropyLoss`）来训练模型，并使用Adam优化器（`optim.Adam`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980b386-c39f-45cb-851a-704410b6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型超参数\n",
    "vocab_size = len(glove.stoi)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_classes = 2  # SST2 是二分类任务\n",
    "\n",
    "# 创建 Transformer 模型\n",
    "model = TransformerTextClassificationModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes)\n",
    "\n",
    "# 创建优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ecc5e-6c5e-49a1-a193-30356b0482f5",
   "metadata": {},
   "source": [
    "### Step 6: 定义训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b548858-3e42-4be4-af6d-93e22523d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for texts, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = predictions.argmax(1)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "    return total_loss / len(dataloader), correct / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10439d7-10f6-4bbe-833a-c8a71921d15e",
   "metadata": {},
   "source": [
    "### Step 7: 训练模型\n",
    "在多个epoch上训练模型，并在每个epoch结束时评估模型在验证集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb563ce-808a-4cfe-8199-89c4c9dc54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "N_EPOCHS = 16\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_model(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_model(model, valid_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}, Validation Loss: {valid_loss:.3f}, Validation Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfae675-d7bf-45d3-a3e6-d0a376bbe039",
   "metadata": {},
   "source": [
    "### Step 8: 测试模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b517d0-2597-4bfe-b115-e234fe893902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估模型\n",
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc244aa9-9b6f-4598-88ae-4c5b2e6d45e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d08a28-38e3-4b0d-a65b-b09ac68f0c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9841a2-b7c3-487c-bd4c-b202d72e43bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a56c74-9a43-4be0-862e-f3c7fdaa7ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
