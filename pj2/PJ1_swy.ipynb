{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4f1da0",
   "metadata": {},
   "source": [
    "## 基于GloVe词向量的文本分类\n",
    "> 宋文彦 \n",
    "> 21302010062"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68522f5",
   "metadata": {},
   "source": [
    "### 代码部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a8f2e",
   "metadata": {},
   "source": [
    "#### 导入torch和其他所需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd94fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/swy_py38/lib/python3.8/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/swy_py38/lib/python3.8/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/swy_py38/lib/python3.8/site-packages/torchtext/transforms.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/swy_py38/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/swy_py38/lib/python3.8/site-packages/torchtext/functional.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.transforms import VocabTransform, ToTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4da663",
   "metadata": {},
   "source": [
    "#### 加载SST数据集\n",
    "提取SST中的训练集 `train_data` 、验证集 `valid_data` 和测试集 `test_data`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a14139d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 8544\n",
      "Number of test samples: 2210\n",
      "Number of validation samples: 1101\n",
      "Train dataset sample: {0.75, 1.0, 0.0, 0.5, 0.25}\n",
      "Validation dataset sample: {0.75, 0.5, 0.25, 1.0, 0.0}\n",
      "Test dataset sample: {0.5, 0.75, 0.0, 1.0, 0.25}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载 SST 数据集\n",
    "dataset = load_dataset('sst', 'default', trust_remote_code=True)\n",
    "\n",
    "# 分割训练、验证、测试集\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(f\"Number of train samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "print(f\"Number of validation samples: {len(valid_data)}\")\n",
    "\n",
    "def round_labels(example):\n",
    "    '''\n",
    "    @description: 将标签值转换为 0, 0.25, 0.5, 0.75, 1\n",
    "    @param: {example} 一个样本\n",
    "    \n",
    "    @return: {example} 处理后的样本\n",
    "    '''\n",
    "    label = example['label']  # 提取出 label 值\n",
    "    if label < 0.2:\n",
    "        example['label'] = 0  # 极负面\n",
    "    elif label < 0.4:\n",
    "        example['label'] = 0.25  # 负面\n",
    "    elif label < 0.6:\n",
    "        example['label'] = 0.5  # 中性\n",
    "    elif label < 0.8:\n",
    "        example['label'] = 0.75  # 正面\n",
    "    else:\n",
    "        example['label'] = 1  # 极正面\n",
    "    return example\n",
    "\n",
    "# 重新处理数据集的标签\n",
    "train_data = train_data.map(round_labels)\n",
    "valid_data = valid_data.map(round_labels)\n",
    "test_data = test_data.map(round_labels)\n",
    "\n",
    "print(\"Train dataset sample:\", set(example['label'] for example in train_data))\n",
    "print(\"Validation dataset sample:\", set(example['label'] for example in valid_data))\n",
    "print(\"Test dataset sample:\", set(example['label'] for example in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38df454",
   "metadata": {},
   "source": [
    "#### 构建文本处理器和标签处理器\n",
    "将GloVe词向量传入文本处理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "989d2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 文本处理器\n",
    "class TextProcessor:\n",
    "    def __init__(self, vocab, max_length=40):\n",
    "        '''\n",
    "        @param vocab: 词汇表\n",
    "        @param max_length: 文本最大长度\n",
    "        '''\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, text):\n",
    "        '''\n",
    "        @param text: 文本\n",
    "        \n",
    "        @return token_ids: 文本的词汇表索引张量\n",
    "        '''\n",
    "        # 将文本分词并映射到词汇表索引\n",
    "        tokens = text.lower().split()\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else 0 for token in tokens]\n",
    "        return torch.tensor(token_ids[:self.max_length])\n",
    "\n",
    "# 标签处理器：将标签转换为张量\n",
    "def label_processor(label):\n",
    "    return torch.tensor(int(label))\n",
    "\n",
    "# 初始化 TextProcessor\n",
    "glove = GloVe(name='6B', dim=100)       # 使用 GloVe 词向量\n",
    "text_processor = TextProcessor(glove.stoi)  # 传入 GloVe 词汇表\n",
    "\n",
    "# 批处理函数\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    @param batch: 一个 batch 的数据\n",
    "\n",
    "    @return texts: 文本张量\n",
    "    '''\n",
    "    texts, labels = zip(*[(example['sentence'], example['label']) for example in batch])\n",
    "    texts = [text_processor(text) for text in texts]\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "378bdef4",
   "metadata": {},
   "source": [
    "#### 定义 GloVe + Transfomer + Pooling + Classifier 的模型\n",
    "基于Transformer和GloVe词向量的文本分类模型。模型的结构包含以下几部分：\n",
    "\n",
    "1. 嵌入层（Embedding Layer）:\n",
    "   - 使用 `nn.Embedding` 将输入的词汇索引映射到对应的词向量。\n",
    "   - 使用预训练的 `GloVe` 词向量进行初始化，确保模型在开始训练时已经具备良好的词向量表示。\n",
    "   - 词向量的权重不可学习，防止在训练过程中被更新。\n",
    "\n",
    "2. `Transformer` 编码器（Transformer Encoder）:\n",
    "   - 使用 `nn.TransformerEncoder` 对文本序列进行编码。`Transformer` 的编码器能够捕捉句子中的长距离依赖关系和上下文信息。\n",
    "   - 该编码器由若干层 `TransformerEncoderLayer` 组成，每一层包含多头自注意力机制和前馈神经网络。\n",
    "   - `embedding_dim` 表示输入的词向量维度，`nhead=2` 表示使用2个注意力头，`dim_feedforward=hidden_dim` 是前馈神经网络的隐藏层维度。\n",
    "\n",
    "3. 池化层（Pooling Layer）:\n",
    "   - 使用 `nn.AdaptiveMaxPool1d(1)` 对 `Transformer` 编码器的输出进行自适应最大池化。池化的作用是从序列的每个时间步中提取出最重要的信息，并减少数据的维度。\n",
    "   - 池化后的输出形状被调整为 `(batch_size, hidden_dim)`，以便传入分类器进行处理。\n",
    "\n",
    "4. 分类器（Classifier）:\n",
    "   - 分类器由两层全连接网络组成：\n",
    "     1. 第一层将池化后的向量映射到隐藏层大小 `hidden_dim`，并通过 `ReLU` 激活函数增加非线性。\n",
    "     2. 第二层将隐藏层的输出映射到类别数 `num_classes`，最终输出每个类别的预测得分。\n",
    "\n",
    "5. 前向传播（Forward Pass）:\n",
    "   - 输入的文本张量经过嵌入层转换为词向量表示，再通过 `Transformer` 编码器进行特征提取，最后经过池化层和分类器，输出每个样本所属类别的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8caf542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        '''\n",
    "        @param vocab_size: 词汇表大小\n",
    "        @param embedding_dim: 词向量维度\n",
    "        @param hidden_dim: 隐藏层维度\n",
    "        @param num_layers: TransformerEncoder 层数\n",
    "        @param num_classes: 类别数量\n",
    "\n",
    "        @return: None\n",
    "        '''\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.embedding.weight.data.copy_(text_processor.vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(embedding_dim, nhead=2, dim_feedforward=hidden_dim), num_layers)\n",
    "        \n",
    "        #self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        '''\n",
    "        @param x: 输入文本张量，形状为 (batch_size, seq_len)\n",
    "\n",
    "        @return: 输出类别张量，形状为 (batch_size, num_classes)\n",
    "        '''\n",
    "        embedded = self.embedding(x)\n",
    "        encoded = self.transformer_encoder(embedded)\n",
    "        pooled = self.pooling(encoded.permute(0, 2, 1))\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        output = self.classifier(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa1fc4",
   "metadata": {},
   "source": [
    "#### 定义 Random + RNN + Pooling + Classifier 的模型\n",
    "\n",
    "定义基于RNN（循环神经网络）的文本分类模型，适用于自然语言处理任务中的文本分类问题。模型包含以下几个核心部分：\n",
    "\n",
    "1. 词嵌入层（Embedding Layer）:\n",
    "    - 使用nn.Embedding将输入的词汇索引映射为词向量。这一步通过将每个单词的索引转换为一个固定大小的向量表示，使得模型能够处理文本数据中的词汇信息。\n",
    "    - 输入为词汇表的大小 `vocab_size`，输出为词向量的维度 `embedding_dim`。\n",
    "\n",
    "2. RNN层（Recurrent Neural Network Layer）:\n",
    "    - 使用 `nn.RNN` 来处理输入的词向量序列，并学习序列中单词的上下文信息。RNN通过循环的方式处理序列数据，适合捕捉句子中的时间依赖关系。\n",
    "    - `embedding_dim` 为输入词向量的维度，`hidden_dim` 为RNN的隐藏状态维度，`num_layers` 表示堆叠RNN层的数量。\n",
    "\n",
    "3. 池化层（Pooling Layer）:\n",
    "    - 使用nn.AdaptiveAvgPool1d(1)进行自适应平均池化操作，将RNN层输出的序列压缩成一个固定大小的向量。这一步简化了时间维度，并保留了RNN输出中的关键信息。\n",
    "\n",
    "4. 分类器（Classifier）:\n",
    "    - 分类器由两个全连接层组成。首先，RNN输出的特征向量通过线性变换映射到隐藏层维度，并经过 `ReLU` 激活函数增加非线性。然后，进一步映射到目标类别数 `num_classes`，输出每个类别的预测概率\n",
    "\n",
    "5. 前向传播（Forward Pass）:\n",
    "    - 在前向传播过程中，输入的文本序列首先通过词嵌入层转换为词向量表示，再经过RNN处理序列信息。随后通过池化层提取全局特征，最后通过分类器输出预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed698f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        '''\n",
    "        @param: vocab_size: 词汇表大小\n",
    "        @param: embedding_dim: 词向量维度\n",
    "        @param: hidden_dim: 隐藏层维度\n",
    "        @param: num_layers: RNN 层数\n",
    "        @param: num_classes: 分类类别数\n",
    "\n",
    "        @output: output: 模型输出   \n",
    "        '''\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \n",
    "        # 词嵌入层：将输入的单词索引转换为对应的词向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # 自适应平均池化层：将RNN输出的时间步维度池化为一个固定大小\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # 分类器：由两层全连接层组成\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        embedded = self.embedding(x)    # 词嵌入\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        pooled = self.pooling(rnn_output.permute(0, 2, 1))\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        output = self.classifier(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6d396",
   "metadata": {},
   "source": [
    "#### 定义模型超参数，创建优化器和损失函数\n",
    "1. 使用 `TextClassificationModel` 类创建一个文本分类模型对象。\n",
    "   1. `vocab_size` 为GloVe的词汇表长度，即使用的单词数量；\n",
    "   2. `embedding_dim` 为词向量的维度；\n",
    "   3. `num_layers` 为编码器的层数；\n",
    "   4. `num_classes` 为任务的类别数。\n",
    "2. 创建优化器 `optimizer`，使用 Adam 优化算法更新模型参数，学习率为0.001。\n",
    "3. 使用 `nn.CrossEntropyLoss` 创建一个交叉熵损失函数对象 `criterion`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b49785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的超参数\n",
    "vocab_size = len(glove.stoi)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_classes = 5\n",
    "\n",
    "# 创建文本分类模型的实例，传入词汇表大小、嵌入维度、隐藏层维度、编码器层数和类别数\n",
    "model = TextClassificationModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_classes)\n",
    "\n",
    "# 创建优化器，定义损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40492b97",
   "metadata": {},
   "source": [
    "#### 模型训练\n",
    "* `optimizer.zero_grad()` 在每个批次开始时，将优化器的梯度缓冲区清零，确保每个批次的梯度计算是独立的。\n",
    "* `loss = criterion(predictions, batch.label)` 计算预测结果与批次的标签之间的损失。\n",
    "* `loss.backward()` 根据损失值，计算模型参数的梯度。\n",
    "* `optimizer.step()` 根据优化器的更新规则，更新模型的参数，以减小损失函数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e93006dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练函数\n",
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    '''\n",
    "    @param: model: 模型\n",
    "    @param: iterator: 数据加载器\n",
    "    @param: optimizer: 优化器\n",
    "\n",
    "    @output: None\n",
    "    '''\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        texts, labels = batch  # 直接解包 tuple\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505422f8",
   "metadata": {},
   "source": [
    "#### 模型验证\n",
    "* 使用 `torch.no_grad()` 以禁用梯度计算，减少内存消耗和计算量。\n",
    "* 返回在整个验证集上的损失和准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8457ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型验证函数\n",
    "def evaluate_model(model, iterator, criterion) -> float:\n",
    "    '''\n",
    "    @param: model: 模型\n",
    "    @param: iterator: 数据加载器\n",
    "    @param: criterion: 损失函数\n",
    "\n",
    "    @output: total_loss: 平均损失\n",
    "    '''\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            texts, labels = batch  # 直接解包 tuple\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = predictions.argmax(1)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "    return total_loss / len(iterator), correct / len(iterator.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f88abb",
   "metadata": {},
   "source": [
    "#### 模型训练\n",
    "* 训练的总Epoch数为16。\n",
    "* 调用已定义的 `train_model` 函数和 `evaluate_model` 函数。\n",
    "* 在每个Epoch结束后，打印当前Epoch的验证集损失和准确率，监控模型在训练过程中的性能，并观察模型是否出现过拟合或欠拟合的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aac7ee3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/ncp146m53fz3rfc_06r2pj_40000gn/T/ipykernel_41418/3771617310.py:29: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\tValidation Loss: 0.412 | Validation Acc: 85.01%\n",
      "Epoch: 02\tValidation Loss: 0.388 | Validation Acc: 84.74%\n",
      "Epoch: 03\tValidation Loss: 0.392 | Validation Acc: 84.20%\n",
      "Epoch: 04\tValidation Loss: 0.406 | Validation Acc: 83.83%\n",
      "Epoch: 05\tValidation Loss: 0.429 | Validation Acc: 83.29%\n",
      "Epoch: 06\tValidation Loss: 0.536 | Validation Acc: 77.57%\n",
      "Epoch: 07\tValidation Loss: 0.539 | Validation Acc: 81.38%\n",
      "Epoch: 08\tValidation Loss: 0.889 | Validation Acc: 75.66%\n",
      "Epoch: 09\tValidation Loss: 0.799 | Validation Acc: 80.65%\n",
      "Epoch: 10\tValidation Loss: 0.802 | Validation Acc: 81.29%\n",
      "Epoch: 11\tValidation Loss: 1.134 | Validation Acc: 79.02%\n",
      "Epoch: 12\tValidation Loss: 1.131 | Validation Acc: 80.84%\n",
      "Epoch: 13\tValidation Loss: 0.957 | Validation Acc: 83.83%\n",
      "Epoch: 14\tValidation Loss: 1.182 | Validation Acc: 83.29%\n",
      "Epoch: 15\tValidation Loss: 1.321 | Validation Acc: 79.29%\n",
      "Epoch: 16\tValidation Loss: 1.273 | Validation Acc: 83.02%\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "EPOCHS = 16\n",
    "for epoch in range(EPOCHS):\n",
    "    train_model(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_model(model, valid_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}\\tValidation Loss: {valid_loss:.3f} | Validation Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a26e3",
   "metadata": {},
   "source": [
    "#### 模型测试与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96c31899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/ncp146m53fz3rfc_06r2pj_40000gn/T/ipykernel_41418/3771617310.py:29: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2790 | Test Acc: 81.58%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swy_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
